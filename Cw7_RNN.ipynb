{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b5b8f8-1563-4fce-9f73-1dc67accc63e",
   "metadata": {},
   "source": [
    "<h4> Zadanie1 (2pkt, RNN/LSTM vs klasyczna reprezentacja)\n",
    "\n",
    "- Rozważ plik all_chem_df.csv, zawierający pewne leki (w formacie SMILES) i informację o obszarze działania. Wybierz 4 najczęstsze targety. W oparciu o reprezentację One-hot-encoding oraz sieci rekurencyjne zbuduj klasyfikator. Wydziel zbiór treningowy i testowy (a być może także walidacyjny)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464dbd9",
   "metadata": {},
   "source": [
    "Cześć I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75395af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e29159-286a-48f8-bdb8-9bf14c7c842d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('antiinfective', 2412), ('antineoplastic', 1175), ('cns', 1149), ('cardio', 797)]\n",
      "['CCC[C@@]1(CCc2ccccc2)CC(O)=C([C@H](CC)c2cccc(NS(=O)(=O)c3ccc(C(F)(F)F)cn3)c2)C(=O)O1', 'CCCCC(C)C(=O)OC1C(C)C(CC)OC2(CC3CC(C/C=C(\\\\C)CC(C)/C=C/C=C4\\\\COC5C(O)C(C)=CC(C(=O)O3)C45O)O2)C1O', 'COc1cc2c(c(OC)c1OC)-c1c(cc3c(c1OC)OCO3)C[C@H](C)[C@@](C)(O)C2', 'CC(=O)OCC(=O)C1CCC2C3CCC4CC(O)CCC4(C)C3C(=O)CC12C', 'CC(=O)Nc1nnc(S(N)(=O)=O)s1']\n",
      "['antiinfective', 'antiinfective', 'antineoplastic', 'cns', 'cns']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"all_chem_df.csv\", sep = \",\")\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(df[\"tags\"]).most_common(4))\n",
    "\n",
    "df2 = df[df['tags'].isin(['antiinfective', 'antineoplastic', 'cns','cardio'])]\n",
    "\n",
    "X = list(df2[\"smiles\"])\n",
    "y = list(df2[\"tags\"])\n",
    "# zlaczone = \"\".join(list(df2[\"smiles\"]))\n",
    "\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6f162ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = sorted(list(set(\"\".join(X))))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(all_letters)} #enumerate(all_letters) daje pary (index, char)\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def text_to_onehot(text):\n",
    "    seq = torch.zeros(len(text), n_letters) #(rows, cols)\n",
    "    for i, ch in enumerate(text):\n",
    "        seq[i, char_to_idx[ch]] = 1.0\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e82ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return text_to_onehot(self.data[idx]), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "#do obslugi sekwencji o roznej dlugosci\n",
    "def collate_fn(batch):\n",
    "    # batch = [(seq1, label1), (seq2, label2), ...]\n",
    "    sequences, labels = zip(*batch) # rozpakowujemy na dwie listy\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences]) #wymiar bez paddingu\n",
    "    padded = pad_sequence(sequences, batch_first=True)  #wymiar: (batch, max_len, vocab_size)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afcf39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(X)) #dzielimy na podstawie indeksów zeby przyśpieszyć podział i stratyfikować\n",
    "train_ind, test_ind = train_test_split(indices, test_size=0.3, random_state=42, stratify=y) #stratify=y - podział z zachowaniem proporcji klas w y\n",
    "\n",
    "#zmiana etykiet na numery bo Pytorch wymaga etykiet numerycznych\n",
    "uni_classes = sorted(list(set(y)))\n",
    "class_to_idx = {c: i for i, c in enumerate(uni_classes)}\n",
    "\n",
    "y_idx = [class_to_idx[c] for c in y]\n",
    "\n",
    "train_targets = [y_idx[i] for i in train_ind]\n",
    "test_targets = [y_idx[i] for i in test_ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c1eddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni, counts = np.unique(train_targets, return_counts=True)\n",
    "weight_per_class = {u: len(train_targets)/c for u, c in zip(uni, counts)}\n",
    "weights = [weight_per_class[c] for c in train_targets]\n",
    "weights = torch.DoubleTensor(weights)  # wymagane przez WeightedRandomSampler, bez tego miałam błąd\n",
    "sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "train_dataset = ListDataset([X[i] for i in train_ind], train_targets)\n",
    "test_dataset = ListDataset([X[i] for i in test_ind], test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, sampler=sampler, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d2046bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3703\n",
      "Epoch 2, Loss: 1.2926\n",
      "Epoch 3, Loss: 1.2604\n",
      "Epoch 4, Loss: 1.3086\n",
      "Top-1 accuracy: 32.05%\n",
      "Top-3 accuracy: 77.17%\n"
     ]
    }
   ],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, hidden = self.rnn(packed)\n",
    "        out = self.fc(hidden.squeeze(0))\n",
    "        return out\n",
    "#budujemy model\n",
    "num_classes = len(set(y))\n",
    "model = RNNClassifier(input_size=n_letters, hidden_size=16, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "#Trening\n",
    "for epoch in range(4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch, lengths in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch, lengths)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate(model, loader, topk=(1,3)):\n",
    "    model.eval()\n",
    "    correct = {k: 0 for k in topk}\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in loader:\n",
    "            outputs = model(X_batch, lengths)\n",
    "            total += y_batch.size(0)\n",
    "            for k in topk:\n",
    "                _, pred = outputs.topk(k, dim=1)\n",
    "                correct[k] += (pred == y_batch.view(-1, 1)).any(dim=1).sum().item()\n",
    "\n",
    "    for k in topk:\n",
    "        acc = 100 * correct[k] / total\n",
    "        print(f\"Top-{k} accuracy: {acc:.2f}%\")\n",
    "\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa5413",
   "metadata": {},
   "source": [
    "Część II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac9b1f",
   "metadata": {},
   "source": [
    "- W pliku drugs_prop.txt występują te same leki, ale tym razem podano ich wybrane własności fizykochemiczne. Zbuduj klasyfikator w oparciu o regresję logistyczną lub/oraz SVM. Wydziel zbiór treningowy i testowy.\n",
    "- Porównaj efektywność zbudowanych modeli. Skomentuj otrzymane wyniki. Wskaż plusy i minusy obydwu rozwiązań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00fdc594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['mass', ' logp', ' h_d', ' h_a', ' rot_b', ' tpsa', ' target'], dtype='object')\n",
      "     mass   logp   h_d   h_a   rot_b    tpsa\n",
      "0  602.68   7.33     2     6      11  105.59\n",
      "1  686.88   5.24     3    10       6  140.98\n",
      "2  416.47   3.60     1     7       4   75.61\n",
      "3  390.52   3.32     1     5       3   80.67\n",
      "4  222.25  -0.86     2     6       2  115.04\n",
      "0      antiinfective\n",
      "1      antiinfective\n",
      "2     antineoplastic\n",
      "3                cns\n",
      "4                cns\n",
      "Name:  target, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_drugs = pd.read_csv(\"drugs_prop.txt\", sep = ',')\n",
    "print(df_drugs.columns)\n",
    "\n",
    "y = df_drugs[' target']\n",
    "X = df_drugs.drop(columns=[' target'])\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb0dd7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  antiinfective       0.71      0.48      0.57       739\n",
      " antineoplastic       0.29      0.23      0.26       340\n",
      "         cardio       0.33      0.36      0.35       247\n",
      "            cns       0.40      0.75      0.52       334\n",
      "\n",
      "       accuracy                           0.47      1660\n",
      "      macro avg       0.44      0.46      0.43      1660\n",
      "   weighted avg       0.51      0.47      0.47      1660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train) #parametry standaryzacji wyznaczane są na zbiorze treningowym\n",
    "X_test_std = stdsc.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "print(classification_report(y_test, lr.predict(X_test_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49d29ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 accuracy: 85.90%\n"
     ]
    }
   ],
   "source": [
    "#top 3 accuracy\n",
    "probs = lr.predict_proba(X_test_std)  #shape: (n_samples, n_classes)\n",
    "top3_preds = np.argsort(probs, axis=1)[:, -3:]  #shape: (n_samples, 3)\n",
    "top3_correct = sum(y_test.iloc[i] in lr.classes_[top3_preds[i]] for i in range(len(y_test)))\n",
    "top3_accuracy = top3_correct / len(y_test)\n",
    "print(f\"Top-3 accuracy: {top3_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1722ec55",
   "metadata": {},
   "source": [
    "Regresja logistyczna wyszła lepiej w obu dokładnościach (top1, top3). Oznacza to, że nie zawsze warto korzystać z bardziej skomplikowanego modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ce35e",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cbda32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 45.42%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SVM\n",
    "svm = SVC(kernel='linear', C=100, random_state=42, class_weight='balanced', probability=True)\n",
    "svm.fit(X_train_std, y_train)            #dopasowanie modelu do danych\n",
    "\n",
    "#dokładność na zbiorze testowym\n",
    "accuracy = svm.score(X_test_std, y_test)\n",
    "print(f\"Test accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d74d0114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Top-3 accuracy: 89.52%\n"
     ]
    }
   ],
   "source": [
    "#przewidywane prawdopodobieństwa\n",
    "probs = svm.predict_proba(X_test_std)# shape: (n_samples, n_classes)\n",
    "\n",
    "#top-3 predykcje\n",
    "top3_preds = np.argsort(probs, axis=1)[:, -3:]  #ostatnie 3 klasy z największym prawdopodobieństwem\n",
    "\n",
    "y_test_np = y_test.to_numpy()\n",
    "\n",
    "#zamiana y_test na indeksy w svm.classes_\n",
    "y_test_idx = np.array([np.where(svm.classes_ == y)[0][0] for y in y_test_np])\n",
    "\n",
    "# Top-3 accuracy\n",
    "top3_correct = np.any(top3_preds == y_test_idx[:, None], axis=1)\n",
    "top3_accuracy = top3_correct.mean() * 100\n",
    "\n",
    "print(f\"SVM Top-3 accuracy: {top3_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2726d",
   "metadata": {},
   "source": [
    "| Model | Top-1 Accuracy | Top-3 Accuracy |\n",
    "|-------|----------------|----------------|\n",
    "| RNN   | 32.05%         | 77.17%         |\n",
    "| Logistic Regression (LR) | 47%          | 85.90%         |\n",
    "| SVM   | 45.42%         | 89.52%         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496bb5f",
   "metadata": {},
   "source": [
    "Wbrew intuicji, najbardziej skomplikowany model (RNN) nie dał najleszych wyników. Wskazuje to na to, że właściwosci fizykochemiczne dają lepsze przewidywania, niż stuktura leku w formacie smiles, LR i SVM dały porównywalne wyniki."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9729814-ea16-4dce-819e-6dc1a632874c",
   "metadata": {},
   "source": [
    "<h4> Zadanie2 (Jak zakodować aminokwasy? Embeddingi)\n",
    "    \n",
    "- Rozważ dane dotyczącece lokalizacji komórkowej wybranych białek (peptydów) - te same co w ćw2 (z regresji logistycznej). Tym razem, zbuduj model w oparciu o sieci rekurencyjne i zaproponowaną przez siebie reprezentację aminokwasów - tzw. embeddingi. Potestuj różne topologie sieci oraz reprezentacje dla danych. Skomentuj otrzymane wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3570381",
   "metadata": {},
   "source": [
    "Dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "674ed721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location\n",
      "CH     227\n",
      "MT     499\n",
      "SP    2697\n",
      "TH      45\n",
      "Name: sequence, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('swissprot_annotated_proteins.tab', sep = \"\\t\", header = None)\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "plik_fasta = \"targetp.fasta\"\n",
    "\n",
    "ids, sqs = [], []\n",
    "\n",
    "for rekord in SeqIO.parse(plik_fasta, \"fasta\"):\n",
    "    ids.append(str(rekord.id))\n",
    "    sqs.append(str(rekord.seq))\n",
    "\n",
    "labs_df = pd.DataFrame(columns=['location', 'sequence'])\n",
    "\n",
    "for id_val, seq_val in zip(ids, sqs):\n",
    "    #filtracja df po id\n",
    "    filtered = df[df[0] == id_val]\n",
    "    if not filtered.empty: \n",
    "        loc = filtered[1].values[0]\n",
    "        if loc != 'Other':\n",
    "            # +nowy wiersz do df\n",
    "            labs_df = pd.concat([labs_df, pd.DataFrame({'location': [loc], 'sequence': [seq_val]})], ignore_index=True)\n",
    "\n",
    "#liczba sekwencji dla każdej lokalizacji\n",
    "print(labs_df.groupby('location')['sequence'].count())\n",
    "\n",
    "X = list(labs_df['sequence'])\n",
    "y = list(labs_df['location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09c2eb",
   "metadata": {},
   "source": [
    "Podstawowy model RNN - one hot encoding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2915d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1721\n",
      "Epoch 2, Loss: 1.0086\n",
      "Epoch 3, Loss: 0.9217\n",
      "Epoch 4, Loss: 0.9750\n",
      "Epoch 5, Loss: 0.8767\n",
      "Accuracy: 38.90%\n"
     ]
    }
   ],
   "source": [
    "all_letters = sorted(list(set(\"\".join(X))))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(all_letters)} #enumerate(all_letters) daje pary (index, char)\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def text_to_onehot(text):\n",
    "    seq = torch.zeros(len(text), n_letters) #(rows, cols)\n",
    "    for i, ch in enumerate(text):\n",
    "        seq[i, char_to_idx[ch]] = 1.0\n",
    "    return seq\n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return text_to_onehot(self.data[idx]), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "#do obslugi sekwencji o roznej dlugosci\n",
    "def collate_fn(batch):\n",
    "    # batch = [(seq1, label1), (seq2, label2), ...]\n",
    "    sequences, labels = zip(*batch) # rozpakowujemy na dwie listy\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences]) #wymiar bez paddingu\n",
    "    padded = pad_sequence(sequences, batch_first=True)  #wymiar: (batch, max_len, vocab_size)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded, labels, lengths\n",
    "\n",
    "indices = np.arange(len(X)) #dzielimy na podstawie indeksów zeby przyśpieszyć podział i stratyfikować\n",
    "train_ind, test_ind = train_test_split(indices, test_size=0.3, random_state=42, stratify=y) #stratify=y - podział z zachowaniem proporcji klas w y\n",
    "\n",
    "#zmiana etykiet na numery bo Pytorch wymaga etykiet numerycznych\n",
    "uni_classes = sorted(list(set(y)))\n",
    "class_to_idx = {c: i for i, c in enumerate(uni_classes)}\n",
    "\n",
    "y_idx = [class_to_idx[c] for c in y]\n",
    "\n",
    "train_targets = [y_idx[i] for i in train_ind]\n",
    "test_targets = [y_idx[i] for i in test_ind]\n",
    "\n",
    "\n",
    "uni, counts = np.unique(train_targets, return_counts=True)\n",
    "weight_per_class = {u: len(train_targets)/c for u, c in zip(uni, counts)}\n",
    "weights = [weight_per_class[c] for c in train_targets]\n",
    "weights = torch.DoubleTensor(weights)  # wymagane przez WeightedRandomSampler, bez tego miałam błąd\n",
    "sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "train_dataset = ListDataset([X[i] for i in train_ind], train_targets)\n",
    "test_dataset = ListDataset([X[i] for i in test_ind], test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, sampler=sampler, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, hidden = self.rnn(packed)\n",
    "        out = self.fc(hidden.squeeze(0))\n",
    "        return out\n",
    "    \n",
    "#budujemy model\n",
    "num_classes = len(set(y))\n",
    "model = RNNClassifier(input_size=n_letters, hidden_size=16, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "#Trening\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch, lengths in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch, lengths)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in loader:\n",
    "            outputs = model(X_batch, lengths)\n",
    "            _, pred = torch.max(outputs, dim=1)  # wybiera klasę z najwyższym prawdopodobieństwem\n",
    "            correct += (pred == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Accuracy: {acc:.2f}%\")\n",
    "\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64519be",
   "metadata": {},
   "source": [
    "2. Model: class weight zamiast sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdc65631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.1752\n",
      "Epoch 2, Loss: 1.2070\n",
      "Epoch 3, Loss: 1.1564\n",
      "Epoch 4, Loss: 1.1369\n",
      "Epoch 5, Loss: 1.1295\n",
      "Accuracy: 54.56%\n"
     ]
    }
   ],
   "source": [
    "all_letters = sorted(list(set(\"\".join(X))))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(all_letters)}\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def text_to_onehot(text):\n",
    "    seq = torch.zeros(len(text), n_letters)\n",
    "    for i, ch in enumerate(text):\n",
    "        seq[i, char_to_idx[ch]] = 1.0\n",
    "    return seq\n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return text_to_onehot(self.data[idx]), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    padded = pad_sequence(sequences, batch_first=True)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded, labels, lengths\n",
    "\n",
    "indices = np.arange(len(X))\n",
    "train_ind, test_ind = train_test_split(indices, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "uni_classes = sorted(list(set(y)))\n",
    "class_to_idx = {c: i for i, c in enumerate(uni_classes)}\n",
    "y_idx = [class_to_idx[c] for c in y]\n",
    "\n",
    "train_targets = [y_idx[i] for i in train_ind]\n",
    "test_targets = [y_idx[i] for i in test_ind]\n",
    "\n",
    "# class weights zamiast sampler\n",
    "class_counts = np.bincount(train_targets)\n",
    "weights = 1.0 / class_counts\n",
    "weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "train_dataset = ListDataset([X[i] for i in train_ind], train_targets)\n",
    "test_dataset = ListDataset([X[i] for i in test_ind], test_targets)\n",
    "\n",
    "# sampler usunięty — zwykłe shuffle=True\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, hidden = self.rnn(packed)\n",
    "        out = self.fc(hidden.squeeze(0))\n",
    "        return out\n",
    "\n",
    "num_classes = len(set(y))\n",
    "model = RNNClassifier(input_size=n_letters, hidden_size=16, num_classes=num_classes)\n",
    "\n",
    "#dodanie class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch, lengths in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch, lengths)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in loader:\n",
    "            outputs = model(X_batch, lengths)\n",
    "            _, pred = torch.max(outputs, dim=1)\n",
    "            correct += (pred == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Accuracy: {acc:.2f}%\")\n",
    "\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc4d4d0",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202461e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 0.7415\n",
      "Epoch 02 | Loss: 0.6946\n",
      "Epoch 03 | Loss: 0.6448\n",
      "Epoch 04 | Loss: 0.6048\n",
      "Epoch 05 | Loss: 0.5810\n",
      "Accuracy: 77.71%\n"
     ]
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (hidden, cell) = self.lstm(packed)\n",
    "        out = self.dropout(hidden[-1])  # ostatnia warstwa, ostatni stan\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "#Budujemy model\n",
    "num_classes = len(set(y))\n",
    "model = LSTMClassifier(input_size=n_letters, hidden_size=16, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "#Trenujemy model\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch, lengths in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch, lengths)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1:02d} | Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "#Ocena modelu\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in loader:\n",
    "            outputs = model(X_batch, lengths)\n",
    "            _, pred = torch.max(outputs, dim=1)  # wybiera klasę z najwyższym prawdopodobieństwem\n",
    "            correct += (pred == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Accuracy: {acc:.2f}%\")\n",
    "\n",
    "evaluate(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
